{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T05:33:28.287622Z",
     "start_time": "2021-05-13T05:33:28.239993Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "train_path = \"../new_train/new_train\"\n",
    "val_path = '../new_val_in/new_val_in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T05:33:28.315612Z",
     "start_time": "2021-05-13T05:33:28.294607Z"
    }
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self): #len(val_dataset)\n",
    "        return len(self.pkl_list)\n",
    "    \n",
    "    def __getitem__(self, idx): #val_dataset[0]\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T05:33:29.075700Z",
     "start_time": "2021-05-13T05:33:28.319689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205942 3200\n"
     ]
    }
   ],
   "source": [
    "init_dataset = ArgoverseDataset(data_path=train_path) \n",
    "val_dataset  = ArgoverseDataset(data_path=val_path) \n",
    "print(len(init_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144160, 61782]\n"
     ]
    }
   ],
   "source": [
    "lengths = [int(len(init_dataset)*0.7) + 1, int(len(init_dataset)*0.3)]\n",
    "#lengths = [int(len(init_dataset)*0.2) + 1, int(len(init_dataset)*0.5), int(len(init_dataset)*0.3)]\n",
    "\n",
    "print(lengths)\n",
    "train_dataset, test_dataset = random_split(init_dataset, lengths)\n",
    "#print(len(small_train_dataset), len(test_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T05:33:29.106777Z",
     "start_time": "2021-05-13T05:33:29.077780Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_sz = 256\n",
    "def my_collate(batch): #[scene['p_in'][scene['car_mask'].reshape(-1) == 1]]\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in']]) for scene in batch]\n",
    "    out = [numpy.dstack([scene['p_out']]) for scene in batch]\n",
    "    inp = torch.FloatTensor(inp) #LongTensor\n",
    "    out = torch.FloatTensor(out) #LongTensor\n",
    "    return [inp, out]\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_sz, shuffle = True, collate_fn=my_collate, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([256, 60, 19, 2])\n",
      "torch.Size([128, 60, 19, 2])\n"
     ]
    }
   ],
   "source": [
    "def my_collate(batch): #[scene['p_in'][scene['car_mask'].reshape(-1) == 1]]\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in']]) for scene in batch]\n",
    "    inp = torch.FloatTensor(inp) #LongTensor\n",
    "    return inp\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=8)\n",
    "\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    print(sample_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 120, hidden_dim = 1024, dropout = 0.2):\n",
    "        super(Encoder_LSTM, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layer = 2\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_dim, \n",
    "                            num_layers = self.hidden_layer, dropout = dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.hidden = self.lstm(x)\n",
    "        \n",
    "        return x, self.hidden\n",
    "    \n",
    "    def self_hidden(self, batch_size):\n",
    "        return (torch.zeros((self.hidden_layer, batch_size, self.hidden_dim)).cuda(),\n",
    "                torch.zeros((self.hidden_layer, batch_size, self.hidden_dim)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 120, hidden_dim = 1024 , dropout = 0.2):\n",
    "        super(Decoder_LSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layer = 2\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_dim, \n",
    "                            num_layers = self.hidden_layer, dropout = dropout)\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_dim, self.input_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_hidden):\n",
    "        x, self.hidden = self.lstm(x, encoder_hidden)\n",
    "        out = self.linear(x.squeeze(0))\n",
    "        \n",
    "        return out, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, input_size = 120, hidden_dim = 1024):\n",
    "        super(seq2seq, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = 0.2\n",
    "        \n",
    "        self.encoder = Encoder_LSTM(input_size = self.input_size, hidden_dim = self.hidden_dim, dropout = self.dropout).cuda()\n",
    "        self.decoder = Decoder_LSTM(input_size = self.input_size, hidden_dim = self.hidden_dim, dropout = self.dropout).cuda()\n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        \n",
    "        batch_sz = 256\n",
    "        epochs = 40\n",
    "        learning_rate = 1e-3\n",
    "        decay = 1e-4\n",
    "        prediction_size = 60\n",
    "        teaching_ratio = 0.7\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "        criterion = nn.MSELoss()      \n",
    "        \n",
    "        losses = np.zeros(epochs)\n",
    "\n",
    "        with trange(epochs) as tr:\n",
    "            for i in tr:\n",
    "                batch_loss = 0.\n",
    "                start = time.time()\n",
    "                print('epoch', i + 1)\n",
    "                \n",
    "                for i_batch, (seq, labels) in enumerate(train_loader):\n",
    "                    \n",
    "                    seq = seq.permute(0, 2, 1, 3).contiguous() #.view(seq.shape[0], seq.shape[1], -1)\n",
    "                    seq = seq.view(seq.shape[0], seq.shape[1], -1)\n",
    "                    seq, labels = seq.cuda(), labels.cuda()\n",
    "\n",
    "                    outputs = torch.zeros(prediction_size, batch_sz, seq.shape[2])\n",
    "                    enc_hidden = self.encoder.self_hidden(batch_sz)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    enc_output, enc_hidden = self.encoder(seq)\n",
    "\n",
    "                    dec_input = seq[:, -19:, :]\n",
    "                    dec_hidden = enc_hidden\n",
    "\n",
    "                    # https://github.com/lkulowski/LSTM_encoder_decoder/blob/master/code/lstm_encoder_decoder.py\n",
    "                    # Using mixed teacher forcing method, other methods are available\n",
    "        \n",
    "                    print(dec_input.shape)\n",
    "                    for t in range(prediction_size):\n",
    "                        dec_output, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
    "                        print(dec_output.shape)\n",
    "                        outputs[:, t] = dec_output\n",
    "\n",
    "                        if random.random() < teaching_ratio:\n",
    "                            dec_input = labels[:, t]\n",
    "                        else:\n",
    "                            dec_input = dec_output\n",
    "\n",
    "                    loss = criterion(labels, outputs)\n",
    "                    batch_loss += loss.item()\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                batch_loss /= batch_size\n",
    "                losses[i] = batch_loss\n",
    "\n",
    "                tr.set_postfix(loss = \"{:.2f}\".format(batch_loss))\n",
    "                \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "torch.Size([256, 19, 120])\n",
      "torch.Size([256, 19, 120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.cuda.FloatTensor{[256, 19, 120]}, size=[60, 120]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-879867bf2258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-91386b308d66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     54\u001b[0m                         \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                         \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteaching_ratio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.cuda.FloatTensor{[256, 19, 120]}, size=[60, 120]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "model = seq2seq()\n",
    "loss = model.train(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(nn.Module):\n",
    "    def __init__(self, input_size = 120, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layer_size = 2\n",
    "        self.hidden_dim = 2880\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_dim, num_layers = self.hidden_layer_size, \n",
    "                            batch_first = True, dropout = dropout)\n",
    "\n",
    "        self.linear = nn.Conv1d(self.hidden_dim, 1200, 1)\n",
    "        \n",
    "        self.linear2 = nn.Conv1d(1200, 120, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward_test(self, x, steps = 30):\n",
    "        fut = []\n",
    "        h = torch.zeros((self.hidden_layer_size, len(x), self.hidden_dim)).cuda()\n",
    "        c = torch.zeros((self.hidden_layer_size, len(x), self.hidden_dim)).cuda()\n",
    "        for num_iter in range(steps):\n",
    "            x, (h, c) = self.lstm(x, (h, c))\n",
    "            x = x[:, -1:]\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.linear(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear2(x)\n",
    "            x = x.transpose(1, 2)\n",
    "            fut.append(x)\n",
    "        fut = torch.cat(fut, 1)\n",
    "        return fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "base_model(\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (lstm): LSTM(120, 2880, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (linear): Conv1d(2880, 1200, kernel_size=(1,), stride=(1,))\n",
      "  (linear2): Conv1d(1200, 120, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = base_model().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay = 1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T07:04:35.704706Z",
     "start_time": "2021-05-13T07:01:30.199929Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Full loss 1 0 678526.8125 678526.8125\n",
      "Test loss 1 0 678371.625 678371.625\n",
      "Full loss 1 100 464275.53125 258485.453125\n",
      "Test loss 1 100 462708.53125 257435.09375\n",
      "Full loss 1 200 284976.90625 108906.0625\n",
      "Test loss 1 200 283946.125 107885.609375\n",
      "Full loss 1 300 176137.078125 97426.9921875\n",
      "Test loss 1 300 176090.859375 99436.171875\n",
      "Full loss 1 400 124141.765625 81083.0390625\n",
      "Test loss 1 400 124808.9296875 83142.6875\n",
      "Full loss 1 500 96712.75 78902.4609375\n",
      "Test loss 1 500 97858.75 81420.0390625\n",
      "Batch val loss: 63965.72\n",
      "Batch Loss: 104.13\n",
      "This epoch took 719.27 seconds\n",
      "epoch 2\n",
      "Full loss 2 0 85143.640625 74814.3515625\n",
      "Test loss 2 0 86416.546875 77901.3984375\n",
      "Full loss 2 100 74260.9609375 66683.2578125\n",
      "Test loss 2 100 75612.9609375 59095.72265625\n",
      "Full loss 2 200 67377.890625 68435.4453125\n",
      "Test loss 2 200 69062.65625 61748.8046875\n",
      "Full loss 2 300 62511.3828125 52657.078125\n",
      "Test loss 2 300 64371.4609375 55813.6875\n",
      "Full loss 2 400 56966.296875 48135.34375\n",
      "Test loss 2 400 58850.14453125 50077.95703125\n",
      "Full loss 2 500 53344.26953125 45414.34765625\n",
      "Test loss 2 500 55337.1875 48541.30859375\n",
      "Batch Loss: 84.36\n",
      "This epoch took 634.52 seconds\n",
      "epoch 3\n",
      "Full loss 3 0 51384.74609375 51396.5390625\n",
      "Test loss 3 0 53772.8046875 56394.76953125\n",
      "Full loss 3 100 49701.73046875 47769.078125\n",
      "Test loss 3 100 51923.23828125 50434.30859375\n",
      "Full loss 3 200 46418.3359375 64032.890625\n",
      "Test loss 3 200 48584.69921875 55986.9765625\n",
      "Full loss 3 300 44672.796875 45056.12890625\n",
      "Test loss 3 300 46850.54296875 47840.203125\n",
      "Full loss 3 400 42881.4140625 40169.44140625\n",
      "Test loss 3 400 45257.859375 43224.2578125\n",
      "Full loss 3 500 41607.83203125 52675.19140625\n",
      "Test loss 3 500 44094.69140625 56717.5859375\n",
      "Batch val loss: 45834.23\n",
      "Batch Loss: 51.75\n",
      "This epoch took 720.41 seconds\n",
      "epoch 4\n",
      "Full loss 4 0 41235.859375 40690.26953125\n",
      "Test loss 4 0 43649.98046875 40198.125\n",
      "Full loss 4 100 40191.5234375 35478.6328125\n",
      "Test loss 4 100 42911.53515625 37083.859375\n",
      "Full loss 4 200 40333.87890625 38210.8203125\n",
      "Test loss 4 200 42946.8125 48093.20703125\n",
      "Full loss 4 300 39090.89453125 37125.8203125\n",
      "Test loss 4 300 41864.49609375 38114.96484375\n",
      "Full loss 4 400 38487.85546875 38732.60546875\n",
      "Test loss 4 400 40852.5390625 34738.64453125\n",
      "Full loss 4 500 37138.08984375 37135.109375\n",
      "Test loss 4 500 39650.1484375 40324.00390625\n",
      "Batch Loss: 61.51\n",
      "This epoch took 633.87 seconds\n",
      "epoch 5\n",
      "Full loss 5 0 36673.90625 41556.82421875\n",
      "Test loss 5 0 39164.171875 43667.8671875\n",
      "Full loss 5 100 34735.359375 35650.59375\n",
      "Test loss 5 100 37067.8046875 39569.0078125\n",
      "Full loss 5 200 34374.79296875 36934.44921875\n",
      "Test loss 5 200 36850.01953125 39802.703125\n",
      "Full loss 5 300 33571.234375 26838.453125\n",
      "Test loss 5 300 35927.01953125 32928.10546875\n",
      "Full loss 5 400 32077.4375 33238.30859375\n",
      "Test loss 5 400 34519.64453125 38149.30859375\n",
      "Full loss 5 500 32251.103515625 28653.630859375\n",
      "Test loss 5 500 34832.5234375 30971.287109375\n",
      "Batch val loss: 32549.71\n",
      "Batch Loss: 96.86\n",
      "This epoch took 716.43 seconds\n",
      "epoch 6\n",
      "Full loss 6 0 33770.28125 40401.5546875\n",
      "Test loss 6 0 36436.5078125 43879.52734375\n",
      "Full loss 6 100 33745.34765625 29697.3125\n",
      "Test loss 6 100 36568.3671875 35180.21484375\n",
      "Full loss 6 200 31569.556640625 26335.849609375\n",
      "Test loss 6 200 34073.8671875 25772.859375\n",
      "Full loss 6 300 31280.859375 30832.7265625\n",
      "Test loss 6 300 34054.60546875 34153.23828125\n",
      "Full loss 6 400 30516.30078125 35538.79296875\n",
      "Test loss 6 400 33412.83203125 39373.328125\n",
      "Full loss 6 500 29684.912109375 25564.2265625\n",
      "Test loss 6 500 32409.0234375 25748.30859375\n",
      "Batch Loss: 62.16\n",
      "This epoch took 633.24 seconds\n",
      "epoch 7\n",
      "Full loss 7 0 29447.90625 33924.08203125\n",
      "Test loss 7 0 32095.353515625 30769.88671875\n",
      "Full loss 7 100 30068.087890625 29435.734375\n",
      "Test loss 7 100 32769.3203125 37266.625\n",
      "Full loss 7 200 28848.732421875 30508.71484375\n",
      "Test loss 7 200 31528.912109375 30411.220703125\n",
      "Full loss 7 300 29379.64453125 29459.650390625\n",
      "Test loss 7 300 32219.171875 38327.4375\n",
      "Full loss 7 400 28355.369140625 27608.07421875\n",
      "Test loss 7 400 30862.93359375 27881.357421875\n",
      "Full loss 7 500 27348.013671875 32035.203125\n",
      "Test loss 7 500 29790.84375 35297.1171875\n",
      "Batch val loss: 16863.05\n",
      "Batch Loss: 45.77\n",
      "This epoch took 706.81 seconds\n",
      "epoch 8\n",
      "Full loss 8 0 26190.45703125 24728.365234375\n",
      "Test loss 8 0 28422.439453125 28022.3046875\n",
      "Full loss 8 100 25432.8359375 24570.55078125\n",
      "Test loss 8 100 27752.8046875 27151.787109375\n",
      "Full loss 8 200 25305.796875 21299.642578125\n",
      "Test loss 8 200 27620.84765625 26173.357421875\n",
      "Full loss 8 300 25914.71484375 22015.65234375\n",
      "Test loss 8 300 28370.671875 22700.73046875\n",
      "Full loss 8 400 25919.123046875 27581.31640625\n",
      "Test loss 8 400 28552.435546875 35382.4609375\n",
      "Full loss 8 500 25907.2890625 27531.58203125\n",
      "Test loss 8 500 28643.5078125 27614.8203125\n",
      "Batch Loss: 51.92\n",
      "This epoch took 632.44 seconds\n",
      "epoch 9\n",
      "Full loss 9 0 26728.1953125 30640.41796875\n",
      "Test loss 9 0 29625.748046875 31684.8203125\n",
      "Full loss 9 100 27192.10546875 27259.57421875\n",
      "Test loss 9 100 29999.3359375 30176.568359375\n",
      "Full loss 9 200 25219.236328125 25229.7109375\n",
      "Test loss 9 200 27651.1015625 24577.90625\n",
      "Full loss 9 300 24449.28125 22446.5078125\n",
      "Test loss 9 300 27058.83203125 29117.181640625\n",
      "Full loss 9 400 25055.626953125 20088.4453125\n",
      "Test loss 9 400 27670.8359375 22674.666015625\n",
      "Full loss 9 500 25136.65625 29546.791015625\n",
      "Test loss 9 500 27936.03515625 29818.623046875\n",
      "Batch val loss: 22431.35\n",
      "Batch Loss: 72.95\n",
      "This epoch took 728.58 seconds\n",
      "epoch 10\n",
      "Full loss 10 0 25343.8359375 34772.6484375\n",
      "Test loss 10 0 28036.80859375 39477.39453125\n",
      "Full loss 10 100 24979.35546875 23689.05859375\n",
      "Test loss 10 100 27624.32421875 22958.17578125\n",
      "Full loss 10 200 24167.701171875 26513.62890625\n",
      "Test loss 10 200 26627.712890625 22875.083984375\n",
      "Full loss 10 300 24157.01171875 26359.837890625\n",
      "Test loss 10 300 26806.7421875 33420.3984375\n",
      "Full loss 10 400 22983.6640625 18373.52734375\n",
      "Test loss 10 400 25233.990234375 21863.357421875\n",
      "Full loss 10 500 23077.921875 28434.619140625\n",
      "Test loss 10 500 25436.287109375 28696.28125\n",
      "Batch Loss: 28.31\n",
      "This epoch took 633.97 seconds\n",
      "epoch 11\n",
      "Full loss 11 0 22478.083984375 16684.490234375\n",
      "Test loss 11 0 24752.89453125 20013.5703125\n",
      "Full loss 11 100 22295.375 21085.125\n",
      "Test loss 11 100 24416.1796875 23991.5859375\n",
      "Full loss 11 200 21274.546875 17892.513671875\n",
      "Test loss 11 200 23427.5390625 20847.78125\n",
      "Full loss 11 300 20156.388671875 16376.7646484375\n",
      "Test loss 11 300 22267.611328125 19569.642578125\n",
      "Full loss 11 400 19553.4140625 21744.966796875\n",
      "Test loss 11 400 21630.552734375 20260.98828125\n",
      "Full loss 11 500 19889.091796875 17536.486328125\n",
      "Test loss 11 500 22097.146484375 18015.462890625\n",
      "Batch val loss: 12984.02\n",
      "Batch Loss: 51.57\n",
      "This epoch took 734.07 seconds\n",
      "epoch 12\n",
      "Full loss 12 0 20028.265625 22520.169921875\n",
      "Test loss 12 0 22175.26953125 25830.57421875\n",
      "Full loss 12 100 20339.51171875 20730.3828125\n",
      "Test loss 12 100 22551.0390625 20723.712890625\n",
      "Full loss 12 200 21613.1640625 24258.177734375\n",
      "Test loss 12 200 23817.984375 30311.021484375\n",
      "Full loss 12 300 22290.578125 21262.080078125\n",
      "Test loss 12 300 24675.791015625 26279.583984375\n",
      "Full loss 12 400 23149.9375 18946.962890625\n",
      "Test loss 12 400 25631.896484375 21847.40625\n",
      "Full loss 12 500 21739.408203125 19379.43359375\n",
      "Test loss 12 500 24025.078125 23620.275390625\n",
      "Batch Loss: 44.12\n",
      "This epoch took 632.83 seconds\n",
      "epoch 13\n",
      "Full loss 13 0 21683.482421875 18499.193359375\n",
      "Test loss 13 0 24100.984375 21596.025390625\n",
      "Full loss 13 100 21650.556640625 18064.3984375\n",
      "Test loss 13 100 24291.88671875 20406.45703125\n",
      "Full loss 13 200 20975.970703125 16460.296875\n",
      "Test loss 13 200 23690.96484375 20590.59765625\n",
      "Full loss 13 300 20792.525390625 18182.818359375\n",
      "Test loss 13 300 23412.390625 17890.94140625\n",
      "Full loss 13 400 19311.654296875 17470.486328125\n",
      "Test loss 13 400 21778.67578125 18547.421875\n",
      "Full loss 13 500 20154.265625 18772.630859375\n",
      "Test loss 13 500 22802.13671875 21714.236328125\n",
      "Batch val loss: 17902.84\n",
      "Batch Loss: 44.52\n",
      "This epoch took 721.93 seconds\n",
      "epoch 14\n",
      "Full loss 14 0 19967.109375 22847.115234375\n",
      "Test loss 14 0 22474.048828125 34781.79296875\n",
      "Full loss 14 100 20024.806640625 17810.185546875\n",
      "Test loss 14 100 22758.61328125 20597.34375\n",
      "Full loss 14 200 19717.890625 18461.89453125\n",
      "Test loss 14 200 22262.482421875 19587.720703125\n",
      "Full loss 14 300 19695.076171875 20859.34765625\n",
      "Test loss 14 300 22237.720703125 20933.044921875\n",
      "Full loss 14 400 19510.396484375 20062.18359375\n",
      "Test loss 14 400 21909.041015625 23174.4296875\n",
      "Full loss 14 500 19011.9765625 21419.8984375\n",
      "Test loss 14 500 21194.587890625 22265.142578125\n",
      "Batch Loss: 31.47\n",
      "This epoch took 651.41 seconds\n",
      "epoch 15\n",
      "Full loss 15 0 19367.23046875 19366.08984375\n",
      "Test loss 15 0 21855.109375 22594.203125\n",
      "Full loss 15 100 19600.9609375 17322.70703125\n",
      "Test loss 15 100 22026.7734375 19664.693359375\n",
      "Full loss 15 200 19017.09765625 24217.314453125\n",
      "Test loss 15 200 21394.353515625 23563.208984375\n",
      "Full loss 15 300 19494.72265625 22460.853515625\n",
      "Test loss 15 300 21874.5234375 24015.994140625\n",
      "Full loss 15 400 19805.712890625 20943.6796875\n",
      "Test loss 15 400 22321.08203125 24596.69921875\n",
      "Full loss 15 500 20348.599609375 17407.87890625\n",
      "Test loss 15 500 23121.80859375 16299.6845703125\n",
      "Batch val loss: 15259.11\n",
      "Batch Loss: 26.25\n",
      "This epoch took 747.92 seconds\n",
      "epoch 16\n",
      "Full loss 16 0 20685.255859375 20173.046875\n",
      "Test loss 16 0 23795.849609375 20301.240234375\n",
      "Full loss 16 100 20733.552734375 24196.638671875\n",
      "Test loss 16 100 23613.3515625 24437.41015625\n",
      "Full loss 16 200 19710.783203125 17743.689453125\n",
      "Test loss 16 200 22365.357421875 22611.38671875\n",
      "Full loss 16 300 20465.333984375 21164.220703125\n",
      "Test loss 16 300 23485.779296875 24201.26171875\n",
      "Full loss 16 400 20331.076171875 17477.919921875\n",
      "Test loss 16 400 23433.634765625 20857.3671875\n",
      "Full loss 16 500 19323.884765625 17684.921875\n",
      "Test loss 16 500 22016.560546875 21127.380859375\n",
      "Batch Loss: 29.34\n",
      "This epoch took 633.03 seconds\n",
      "epoch 17\n",
      "Full loss 17 0 18526.759765625 20460.685546875\n",
      "Test loss 17 0 21072.279296875 25065.373046875\n",
      "Full loss 17 100 18617.974609375 18174.27734375\n",
      "Test loss 17 100 21026.826171875 21586.732421875\n",
      "Full loss 17 200 18782.255859375 20575.076171875\n",
      "Test loss 17 200 21301.693359375 23001.01953125\n",
      "Full loss 17 300 19618.5625 22790.734375\n",
      "Test loss 17 300 22574.6484375 21766.08984375\n",
      "Full loss 17 400 19112.62109375 20126.759765625\n",
      "Test loss 17 400 21938.4375 25253.59765625\n",
      "Full loss 17 500 19082.759765625 16960.720703125\n",
      "Test loss 17 500 21969.05859375 18595.34375\n",
      "Batch val loss: 9934.84\n",
      "Batch Loss: 17.57\n",
      "This epoch took 777.65 seconds\n",
      "epoch 18\n",
      "Full loss 18 0 18636.03515625 15758.8525390625\n",
      "Test loss 18 0 21267.654296875 17420.08203125\n",
      "Full loss 18 100 18015.55859375 15199.72265625\n",
      "Test loss 18 100 20638.501953125 16127.2392578125\n",
      "Full loss 18 200 19058.326171875 17980.525390625\n",
      "Test loss 18 200 21739.94921875 20578.515625\n",
      "Full loss 18 300 18576.849609375 16640.740234375\n",
      "Test loss 18 300 21166.26171875 19165.6015625\n",
      "Full loss 18 400 19116.162109375 20948.712890625\n",
      "Test loss 18 400 21951.681640625 30150.8203125\n",
      "Full loss 18 500 18221.470703125 18830.28515625\n",
      "Test loss 18 500 20892.912109375 21748.513671875\n",
      "Batch Loss: 23.24\n",
      "This epoch took 649.48 seconds\n",
      "epoch 19\n",
      "Full loss 19 0 17575.603515625 14826.125\n",
      "Test loss 19 0 20132.1875 17185.029296875\n",
      "Full loss 19 100 17237.630859375 16760.5546875\n",
      "Test loss 19 100 19690.162109375 16891.748046875\n",
      "Full loss 19 200 17683.099609375 15846.4541015625\n",
      "Test loss 19 200 20356.798828125 19331.861328125\n",
      "Full loss 19 300 17826.8671875 16873.607421875\n",
      "Test loss 19 300 20590.9921875 21379.130859375\n",
      "Full loss 19 400 17054.39453125 18545.095703125\n",
      "Test loss 19 400 19566.240234375 21119.109375\n",
      "Full loss 19 500 16335.705078125 17609.1953125\n",
      "Test loss 19 500 18663.08203125 20718.44921875\n",
      "Batch val loss: 10962.91\n",
      "Batch Loss: 35.99\n",
      "This epoch took 801.95 seconds\n",
      "epoch 20\n",
      "Full loss 20 0 16754.01171875 15091.623046875\n",
      "Test loss 20 0 19278.5390625 17182.3046875\n",
      "Full loss 20 100 16710.462890625 17412.41015625\n",
      "Test loss 20 100 19168.66015625 22617.18359375\n",
      "Full loss 20 200 16592.462890625 17171.421875\n",
      "Test loss 20 200 19247.322265625 22394.494140625\n",
      "Full loss 20 300 17680.8671875 18193.501953125\n",
      "Test loss 20 300 20428.751953125 19364.197265625\n",
      "Full loss 20 400 17405.73046875 16707.240234375\n",
      "Test loss 20 400 20129.00390625 19134.482421875\n",
      "Full loss 20 500 16936.23828125 18592.5625\n",
      "Test loss 20 500 19554.92578125 19157.85546875\n",
      "Batch Loss: 23.59\n",
      "This epoch took 659.64 seconds\n",
      "epoch 21\n",
      "Full loss 21 0 16989.69921875 14804.9404296875\n",
      "Test loss 21 0 19789.267578125 20746.662109375\n",
      "Full loss 21 100 16253.2529296875 12886.16015625\n",
      "Test loss 21 100 18972.408203125 14846.083984375\n",
      "Full loss 21 200 16086.9755859375 13544.7607421875\n",
      "Test loss 21 200 18717.0078125 16346.34765625\n",
      "Full loss 21 300 15780.94921875 18808.197265625\n",
      "Test loss 21 300 18482.8984375 21496.03125\n",
      "Full loss 21 400 16209.541015625 17569.423828125\n",
      "Test loss 21 400 18982.89453125 19197.44140625\n",
      "Full loss 21 500 16575.90625 18170.857421875\n",
      "Test loss 21 500 19240.890625 19713.40625\n",
      "Batch val loss: 8671.66\n",
      "Batch Loss: 21.57\n",
      "This epoch took 821.04 seconds\n",
      "epoch 22\n",
      "Full loss 22 0 16243.96875 17149.0\n",
      "Test loss 22 0 19207.498046875 20081.392578125\n",
      "Full loss 22 100 16369.98828125 14896.23828125\n",
      "Test loss 22 100 19143.982421875 16707.705078125\n",
      "Full loss 22 200 17429.580078125 17582.447265625\n",
      "Test loss 22 200 20468.83203125 19940.70703125\n",
      "Full loss 22 300 18594.2734375 17485.68359375\n",
      "Test loss 22 300 21935.73828125 20909.375\n",
      "Full loss 22 400 18442.81640625 17637.8125\n",
      "Test loss 22 400 21712.46484375 20777.970703125\n",
      "Full loss 22 500 18252.2578125 18719.958984375\n",
      "Test loss 22 500 21316.1484375 21869.759765625\n",
      "Batch Loss: 38.35\n",
      "This epoch took 648.48 seconds\n",
      "epoch 23\n",
      "Full loss 23 0 17847.791015625 16488.3125\n",
      "Test loss 23 0 20613.1015625 19868.66015625\n",
      "Full loss 23 100 17957.125 16861.740234375\n",
      "Test loss 23 100 20787.560546875 18567.716796875\n",
      "Full loss 23 200 17109.740234375 20735.431640625\n",
      "Test loss 23 200 19779.791015625 23153.53515625\n",
      "Full loss 23 300 17976.52734375 14947.2666015625\n",
      "Test loss 23 300 20601.865234375 17115.943359375\n",
      "Full loss 23 400 18221.81640625 20734.693359375\n",
      "Test loss 23 400 21033.306640625 21509.189453125\n",
      "Full loss 23 500 18523.91015625 18792.68359375\n",
      "Test loss 23 500 21461.416015625 21999.876953125\n",
      "Batch val loss: 11615.79\n",
      "Batch Loss: 44.59\n",
      "This epoch took 1496.31 seconds\n",
      "epoch 24\n",
      "Full loss 24 0 18111.3671875 20539.576171875\n",
      "Test loss 24 0 21106.087890625 25051.431640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-32a4e162e355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "loss_ema = -1\n",
    "loss_ema_test = -1\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    start = time.time()\n",
    "    print('epoch', i + 1)\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        losses = 0\n",
    "        model.train()\n",
    "        seq, labels = sample_batch\n",
    "        seq = seq.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        full = torch.cat([seq, labels], 2).transpose(1, 2).reshape((-1, 49, 120)).float()\n",
    "        \n",
    "        y_pred = model(full[:, :-1])[:, -30:]\n",
    "        y_pred = y_pred.reshape((-1, 30, 60, 2)).transpose(1, 2)\n",
    "        \n",
    "        loss = criterion(labels, y_pred) #(torch.mean(y_pred - labels) ** 2)** 0.5\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss_ema < 0:\n",
    "            loss_ema = loss\n",
    "        loss_ema = loss_ema*0.99 + loss * 0.01\n",
    "        losses += loss.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model.forward_test(seq.transpose(1,2).reshape(-1, 19, 120))\n",
    "            y_pred_test = y_pred_test.reshape((-1, 30, 60, 2)).transpose(1, 2)\n",
    "            loss_test= criterion(y_pred_test, labels) #(torch.mean(y_pred_test - labels) ** 2)** 0.5\n",
    "            if loss_ema_test < 0:\n",
    "                loss_ema_test = loss_test\n",
    "            loss_ema_test = loss_ema_test*0.99 + loss_test * 0.01\n",
    "        \n",
    "        if i_batch % 100 == 0:\n",
    "            print(\"Full loss\", i + 1, i_batch, loss_ema.item(), loss.item())\n",
    "            \n",
    "            print(\"Test loss\", i + 1, i_batch, loss_ema_test.item(), loss_test.item())\n",
    "            \n",
    "    if i % 2 == 0:\n",
    "        total_val_loss = 0\n",
    "        model.eval()\n",
    "        for i_batch, sample_batch in enumerate(test_loader):\n",
    "            seq, labels = sample_batch\n",
    "            seq = seq.cuda()\n",
    "            labels = labels.cuda()\n",
    "            with torch.no_grad():\n",
    "                y_pred_test = model.forward_test(seq.transpose(1,2).reshape(-1, 19, 120))\n",
    "                y_pred_test = y_pred_test.reshape((-1, 30, 60, 2)).transpose(1, 2)\n",
    "                loss_val= criterion(y_pred_test, labels) #(torch.mean(y_pred_test - labels) ** 2)** 0.5\n",
    "            total_val_loss += loss_val\n",
    "        batch_val_loss = total_val_loss / len(test_loader)\n",
    "        print(\"Batch val loss: {:.2f}\".format(batch_val_loss))\n",
    "            \n",
    "    batch_loss = losses / len(train_loader)\n",
    "    print(\"Batch Loss: {:.2f}\".format(batch_loss))\n",
    "    loss_list.append(batch_loss)\n",
    "    end = time.time()\n",
    "    print(\"This epoch took {:.2f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-c7747a5590e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticklabel_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sci\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscilimits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss over epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2103\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2106\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 _png.write_png(renderer._renderer, fh, self.figure.dpi,\n\u001b[1;32m    537\u001b[0m                                metadata={**default_metadata, **metadata})\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3ib5bn48e8tb3nGe8VOYjtOHMgihB0SCBRo2bSFtrSsUkbpoKfz15729Jz2tKWndLBb9qaFAGWvhJEwMglZdpxtJ96JZzz1/P7QKyMcD9nWtO7PdfnCevXq1S3h6Naz7keMMSillApftkAHoJRSKrA0ESilVJjTRKCUUmFOE4FSSoU5TQRKKRXmNBEopVSY00Sg/EJEXhaRb3j73FHGsFhEqrx9XQUi8oCI/E+g41BjExnoAFTwEpE2t5t2oAvos25/yxjzqKfXMsac7YtzlVLjp4lADckYk+D6XUR2A9cYY94YeJ6IRBpjev0Zm3LS9155g3YNqVFzdbGIyI9FpAa4X0QmicgLIlIvIget3/PdHrNCRK6xfr9CRN4TkT9a5+4SkbPHeO5UEXlHRFpF5A0RuV1EHvHwdcy0nuuQiGwWkfPc7jtHRLZY160Wkf+wjqdbr+2QiDSJyLsiMui/IxE5UURWi0iz9d8TreOXisiaAed+X0Set36PsV7vXhGpFZG7RCRuqPd+iOe+SkS2Wu/ZqyJS6HafEZHviMhOEWkQkVtcr0FEbCLycxHZIyJ1IvKQiCS7PfZkEVllvf59InKF29NOEpEXrffsQxEpsh4jInKrdb1mEdkoIkd58v9I+YcmAjVW2UAqUAhci/Nv6X7rdgFwGLhtmMcfB5QD6cAfgHtFRMZw7mPAR0Aa8Cvgck+CF5Eo4N/Aa0AmcBPwqIiUWqfci7P7KxE4CnjLOv4DoArIALKAnwFH1GkRkVTgReCvVmx/Al4UkTTgeaBURErcHvIV67UA/B6YDswFioE84D/dzh343g987gusuC6y4nwXeHzAaRcCC4D5wPnAVdbxK6yfJcA0IAHr/6OIFAAvA3+zrjsX2OB2zcuA/wImAZXAb6zjZwKLrNeUAnwZaBwYtwogY0zI/QD3AXXAJg/OvRnYAmwE3gQKAx1/KP4Au4Gl1u+LgW4gdpjz5wIH3W6vwNm1BM4Pmkq3++w4P0yzR3MuzoTTC9jd7n8EeGSImBYDVdbvpwA1gM3t/seBX1m/7wW+BSQNuMavgeeA4hHer8uBjwYcex+4wi3O/7R+LwFardcmQDtQ5Pa4E4Bdo3jvXwaudrttAzpcf/vW+3eW2/03AG9av78J3OB2XynQg7Mb+afAsiGe8wHgH263zwG2Wb+fBlQAx7u/3/oTPD+h2iJ4ADjLw3PXAwuMMbOBf+H8RqnGr94Y0+m6ISJ2Ebnb6lJoAd4BUkQkYojH17h+McZ0WL8mjPLcXKDJ7RjAPg/jzwX2GWMcbsf24Pz2DXAxzg+zPSLytoicYB2/Bee33desrpWfDHP9PQOOuV//MZzfoMHZGnjWeh0ZOBPCWqv75RDwinXc5TPv/SAKgb+4Pb4JZ4LJczvH/X3aY8U7WNx7cCaBLGAysGOY561x+70D6/+nMeYtnK2K24FaEblHRJKGuY7ys5BMBMaYd3D+cfcTkSIReUVE1lr9tjOsc5e7fVB8AOSjvGFgd8gPcH57PM4Yk4SzKwCcH0C+cgBIFRG727HJHj52PzB5QP9+AVANYIxZbYw5H2e30bPAU9bxVmPMD4wx04BzgZtF5PQhrl844Fj/9XF2SaWLyFycCcHVLdSAs1ttljEmxfpJNm4D9wzSFTXAPpzdWiluP3HGmFVu57i/TwVWvIPF7Wp11VrXLRrhuQdljPmrMeYYYBbOLqIfjuU6yjdCMhEM4R7gJuuP7T+AOwY552qczWblfYk4P8AOWf3jv/T1Expj9gBrgF+JSLT1rf1cDx/+Ic4umB+JSJSILLYe+4R1ra+KSLIxpgdowZo2KyJfEJFia4zCdbxvkOu/BEwXka+ISKSIfBkoA16wYu/F2UK9BWd//+vWcQfwd+BWEcm0njNPRD43irfmLuCnIjLLenyyiHxxwDk/FOcA/2Tgu8CT1vHHge+LcxA+Afgt8KQV76PAUhH5kvWa0qxENiwROVZEjrPGZdqBTgZ/z1SATIhEYP3Bngj8U0Q2AHcDOQPO+RrOwbFb/B9hWPgzEIfzG+0HOLsz/OGrOPvQG4H/wfmB1jXSg4wx3cB5wNk4Y74D+LoxZpt1yuXAbqub6zrga9bxEuANoA1nn/8dxpgVg1y/EfgCzpZSI/Aj4AvGmAa30x4DlgL/NJ+dAvpjnN1PH1jP/wbO1pZHjDHLcA44P2E9fpP1Ot09B6zFOdj7Is7BcXCOvz2Ms2tvF84P7Zus6+7F2V32A5wt8g3AHA9CSsKZ3A7i7GpqBP7o6etRvifGhObGNCIyBXjBGHOU1d9YbozJGeLcpThnOpxqjKnzX5TK30TkSZyDlD5vkYQqETFAiTGmMtCxqOAwIVoExpgWYJer+WvNW55j/T4PZwvhPE0CE4/V7VBkzX8/C+dUyGcDHZdSoSQkE4GIPI6zWV4qzsU1V+PsIrhaRD4GNuP8QABnV1ACVreRWIt21ISRjXO6aRvOOfvXG2PWBzQipUJMyHYNKaWU8o6QbBEopZTynpArOpeenm6mTJkS6DCUUiqkrF27tsEYkzHYfSGXCKZMmcKaNWtGPlEppVQ/ERm40r2fdg0ppVSY00SglFJhThOBUkqFOU0ESikV5jQRKKVUmNNEoJRSYU4TgVJKhbmwSQQVta389wtb6OrVMuhKKeUubBJB1cEO7n1vFx/sbBr5ZKWUCiNhkwhOLEonLiqC17fUjHyyUkqFkbBJBLFRESyans4bW+rQiqtKKfWpsEkEAGeUZVPT0snm/S2BDkUppYJGWCWC02ZkYhN4bUttoENRSqmgEVaJIDU+mgWFqbyhiUAppfqFVSIAWFqWyZYDLVQd7Ah0KEopFRR8lghE5D4RqRORTUPcLyLyVxGpFJGNIjLfV7G4WzozC4A3t+o+9kopBb5tETwAnDXM/WcDJdbPtcCdPoyl37SMBIoy4nldu4eUUgrwYSIwxrwDDLd663zgIeP0AZAiIjm+isfd0rIsPtjZSEtnjz+eTimlglogxwjygH1ut6usY0cQkWtFZI2IrKmvrx/3E59ZlkWvw7CifPzXUkqpUBfIRCCDHBt0pZcx5h5jzAJjzIKMjEH3Xh6VuZMnkRYfrbOHlFKKwCaCKmCy2+18YL8/njjCJpw+M5Pl5XX09Dn88ZRKKRW0ApkInge+bs0eOh5oNsYc8NeTL52ZRWtnLx/t0iJ0SqnwFumrC4vI48BiIF1EqoBfAlEAxpi7gJeAc4BKoAO40lexDOaUkgxiIm28vqWWk4rT/fnUSikVVHyWCIwxl41wvwFu9NXzjyQuOoJTStJ5fUstvzy3DJHBhiyUUmriC7uVxe6Wzsyi+tBhttW0BjqUcTHGcMX9H/HqZi2xrZQavbBOBKfPzEKEkF9ctr+5kxXl9by6SROBUmr0wjoRZCTGMHdyCm9sDe1EUGG1aEK9ZaOUCoywTgQAZ5RlsbGqmZrmzkCHMmbltc4EUFnfRq9Oh1VKjZImAqsIXSi3Clwtgu5eB7sbtaqqUmp0wj4RFGcmMCXNHtqJoK6VjMQYAMq1e0gpNUphnwhEhKUzs1hV2UhbV2+gwxm1Podhe20bZ83KxiZQXqPbcCqlRifsEwE4xwm6+xy8WxF6Rej2NnXQ1evg6PxkpqTH948XKKWUpzQRAMcUTiLFHhWS00hdXUGlWYnMyE7UriGl1KhpIgAiI2ycVprJW+V1ITfrpsJqAZRkJTA9K5E9TR10dIdeF5dSKnA0EVjOKMviUEcPa/YcDHQoo1Je20pBqh17dCQzshMxBirr2gIdllIqhGgisJwyPYPoCFvI7VFQUdPK9KxEAEqzkwBdWKaUGh1NBJaEmEhOLE7j9a21OOvhBb/uXge7GtopzU4AoCDVTmyUTccJlFKjoonAzdKZWexp7AiZrpVdDe30Okx/iyDCJpRkJvaPGyillCc0EbhZaq0yfj1EFpe5poqWZif2HyvNTtSuIaXUqGgicJOdHMvs/OSQmUZaUdNKhE2Ymh7ff6w0K5H61i6a2rsDGJlSKpRoIhhg6cwsNuw7RF1r8BehK69tZWp6PDGREf3HXK0DHSdQSnlKE8EAZ5RlYQy8tbUu0KGMqKK2ldKsxM8cm9GfCLTUhFLKM5oIBpiRnUheSlzQF6Hr6O5lb1NH/0CxS0ZiDCn2KC01oZTymCaCAUSEM8qyeHd7A4e7+wIdzpAq69owhv6poy4iQmmWDhgrpTyniWAQZ5Rl0dXr4N3twVuEzjUGMLBFAM5WTUVNa8ish1BKBZYmgkEsnJpKYmxkUHcPba9rIzrSRmFa/BH3Tc9OpL27j6qDhwMQmVIq1GgiGERUhI0lpZm8ubWOPkdwfqsur2mlJDOBCJsccd8MnTmklBoFTQRDOKMsi8b2bjbsC84idBW1rYN2C8Gn3UU6YKyU8oQmgiGcWppBpE14LQgXlzUf7uFAc+eQiSAxNoq8lDhtESilPKKJYAhJsVEcPy0tKKuRbu8vLZEw5DmlukmNUspDmgiGcUZZFjvq29lZH1xF6FxdPkO1CMCZCHbUt9HdG1ob7Sil/E8TwTBOn5kJEHSzhypqWomPjiAvJW7Ic2ZkJ9LrMOxqaPdjZEqpUKSJYBj5k+yU5STxxpbgKjdRXtvK9OxERI6cMeTiai1s01ITSqkRaCIYwWkzMlmzp4n2ruDZB3h7bdsRNYYGKspIINImujeBUmpEmghGMDs/GYcJnqmYDW1dNLZ3Dzs+ABAdaWNaRrwOGCulRqSJYARluc59gLfsD44ulophSksMNF1rDimlPKCJYAR5KXEkxUay5UBwJIL+GUPDTB11mZGdSNXBw7QFUbeWUir4aCIYgYgwMyeJrUGSCCpqW5lkjyIjIWbEc0uzk/ofo5RSQ9FE4IGZOUlsO9AaFHWHymucpSWGmzHk4hpQ1nECpdRwNBF4oCw3icM9fexpDOycfGMMFbVtn9msfjj5k+KwR0doIlBKDcuniUBEzhKRchGpFJGfDHJ/soj8W0Q+FpHNInKlL+MZq7Ica8A4wN1D+5s7aevq9WigGMBmE6ZnaakJpdTwfJYIRCQCuB04GygDLhORsgGn3QhsMcbMARYD/yci0b6KaayKM51z8gM9TlDRX2PIs0QAzu6h8lrdpEYpNTRftggWApXGmJ3GmG7gCeD8AecYIFGcHd4JQBMQdFNcYqMiKM5MCPgU0v6po5mjSATZiTS1d1Pf1uWrsJRSIc6XiSAP2Od2u8o65u42YCawH/gE+K4x5ogqaSJyrYisEZE19fWB2T7SOXMosF0s5bWtZCXFkGyP8vgxrk1qKmqCq3CeUip4+DIRDDatZWD/xOeADUAuMBe4TUSSjniQMfcYYxYYYxZkZGR4P1IPlOUkUdPSSWMAv1kPtxnNUKZna80hpdTwfJkIqoDJbrfzcX7zd3cl8IxxqgR2ATN8GNOYzbQGjAPVKuhzGI9qDA2UnhBDekK0DhgrpYbky0SwGigRkanWAPClwPMDztkLnA4gIllAKbDThzGN2cwc5wdwoAaM9zZ10NXr6P+GPxql2Ym6qEwpNSSfJQJjTC/wbeBVYCvwlDFms4hcJyLXWaf9N3CiiHwCvAn82BjT4KuYxiMtIYaspJiATSF1faMfbYsAnDWHKmrbcATBgjilVPCJ9OXFjTEvAS8NOHaX2+/7gTN9GYM3lQWw1ITrG31J1sg1hgaakZ3I4Z4+9jZ1MCU93tuhKaVCnK4sHoWy3CQq69ro6u3z+3NX1LZSkGrHHj363O2qORQspbSVUsFFE8EozMxJotcatPW3scwYcinJdLYidMBYKTUYTQSjEKhSE929DnbWtzN9DN1CAPExkRSk2jURKKUGpYlgFArT4omLivD7OMGuhnZ6HWZUpSUGKs1O1K4hpdSgNBGMQoRNmJGT6PdSE/2b0Yyxawics412NbQHZHxDKRXcNBGM0sycJLYcaPFrEbeKmlYibMK0jLHP+CnNTqTPYais01ITSqnP0kQwSmU5SbR29lJ96LDfnrO8tpWp6fHEREaM+RqumkM6TqCUGkgTwSi5Sk34s3toe23rmBaSuZuSHk9UhOg4gVLqCJoIRmlGdiIi/qs5dLi7jz1NHeMaHwCIirBRlJGgLQKl1BE0EYxSfEwkU9Pi2XKg2S/PV1nXhjFQmj22qaPuZmSH5m5lTe3d7GvqCHQYSk1YmgjGwJ97E5T3l5YYX4sAnCWpDzR30ny4Z9zX8qcfP72Rqx5YHegwlJqwNBGMQVluEnubOmjt9P0HakVtK9GRNgpT7eO+Vv8mNSE0TtDZ08e72+udayn6jtizSCnlBZoIxsBVknqbH7pZymtaKc5IIDJi/P+rXDWH/BG3t3yws5HOHge9DsP+Q52BDkepCUkTwRiU5SQD/pk5VFHbOq4Vxe5yk2NJjIns3/s4FKwo/3Rr0j1N7QGMRKmJSxPBGGQlxTDJHuXzRNB8uIcDzZ3jnjHkIiJMD7EB4xXldf1dWnt1wFgpn9BEMAYiQlluElt9vA9wZZ21GY0XZgy5lGYnsq3Gvyujx2pXQzu7Gzu49NjJREfa2NuoiUApX9BEMEYzs5PYVtPq0wHM8hpnOQhvtQjAOWDc0tlLbUuX167pK8u31QFw2owsJk+KY48mAqV8QhPBGJXlJtHd62BXg+/6rStqW4mPjiAvJc5r13QllW0+bs14w/LyOooy4ilIs1OQamePdg0p5ROaCMaoLNf3exOU17RSkpWIiHjtmqFSc6iju5cPdzWxuDQTcJYA39fUERJdWkqFmhETgYgUiUiM9ftiEfmOiKT4PrTgVpSRQHSEzaeJoMILNYYGSrFHk5UUE/SJ4P0djXT3OlhiJYKCVDttXb00tXcHODKlJh5PWgRPA30iUgzcC0wFHvNpVCEgKsJGSVaCz2YONbR10djezXQvTR11Nz0r+DepWV5ehz06gmOnTgKgMM25oE67h5TyPk8SgcMY0wtcCPzZGPN9IMe3YYUGX5aacM3193aLAJzdQ9vr2oJ2pa4xhuXb6jmpOL2/9HaBtbJaZw4p5X2eJIIeEbkM+AbwgnUsynchhY6ynCQa2rqoa/X+itf+Xcm8OHXUpTTbOdC9O0g/VHfUt1F96DCLSzP6j012JQJtESjldZ4kgiuBE4DfGGN2ichU4BHfhhUafLk3QUVtG5PsUWQkxHj92q5WRrDWHFq+zbma2DVQDBAbFUF2UqxOIVXKB0ZMBMaYLcaY7xhjHheRSUCiMeZ3fogt6JVZicAX3UMVta1M9/KMIZeSrARsErw1h5aX11GalXjEtNmCNDt7tcyEUl7nyayhFSKSJCKpwMfA/SLyJ9+HFvyS7VHkpcR5feaQMYaKGu/VGBooNiqCKWnxlAfhWoK2rl5W725i8YyMI+4rSLVri0ApH/CkayjZGNMCXATcb4w5Bljq27BCh3PA2LsfqAeaO2nt6vXKHgRDmZ6VSEVt8G1kv7KygZ4+w+LpmUfcV5hqp661i8PdfQGITKmJy5NEECkiOcCX+HSwWFnKcpPYWd9GZ4/3PpxcA8W+mDHkUpqdyO7G9qD7UF1RXkdCTCQLpkw64r4CawrpvoPaKlDKmzxJBL8GXgV2GGNWi8g0YLtvwwodZTmJOIx3V+q6po5Oz/L+jCGXGdmJGAPb64JnnMA1bfSUknSiBtl/oTAtHkC7h5TyMk8Gi/9pjJltjLneur3TGHOx70MLDf17E3ixe6i8tpWspBhS7NFeu+ZA04Ow1ER5bSs1LZ39q4kHcq0l2NOoA8ZKeZMng8X5IrJMROpEpFZEnhaRfH8EFwryJ8WREBPp1XEC14whX5qSFk9MpC2oEoFr2uippUcOFANMskeRGBOpG9kr5WWedA3dDzwP5AJ5wL+tYwqw2YSZOYleW0vQ5zBU1rX5dHwAIMImlGQlBFWpieXldZTlJJGVFDvo/SJCQZpWIVXK2zxJBBnGmPuNMb3WzwPA4F/ZwpRr5pDDMf7KmPuaOujscfikxtBA07OCZ7ey5sM9rN1zkCWDTBt1V5Bq1zITSnmZJ4mgQUS+JiIR1s/XgEZfBxZKynKSaO/u88psFn/MGHKZkZ1IXWsXB4OgoufKygb6HGbI8QGXgjQ7VQcP0+eFpKuUcvIkEVyFc+poDXAAuMQ6piz9exN4oXvINWOoONN3M4ZcSrOdcQfDCuPl2+pIio1k7uThK5wXpsbT3eegpsX79Z2UCleezBraa4w5zxiTYYzJNMZcYIzZ44/gQsX0rERsglcGjMtrW5mcGkd8TKQXIhve7LxkIm3CivI6nz/XcBwOw4qKehZNzyBykGmj7vrLUevMIaW8ZshPGxH5GzBk+9sY8x2fRBSCYqMiKMpIGPcU0o5uZ3mF2fn+2fdnUnw0i0szWLa+mh9+rnTED2Ff2XKghfrWrhG7heCz5ahPLPJ1ZEqFh+H+5a8B1g7zMyIROUtEykWkUkR+MsQ5i0Vkg4hsFpG3Rxd+8PDG3gR/eXM7tS1dXHPyVC9FNbKL5+dT19rFe5UNfnvOgVwtkqGmjbrLSY4l0iZajlopLxqyRWCMeXA8FxaRCOB24AygClgtIs8bY7a4nZMC3AGcZYzZKyIjfyUMUmW5STz/8X4OdXSPaSHYtpoW7n13F188Jp/jpqX5IMLBnTYzk+S4KJ5ZV/2Zss/+tLy8ntn5yaR7UHI7MsJG/qQ4nUKqlBf5si9gIVBprUTuBp4Azh9wzleAZ4wxewGMMYHtrB6HmeMoSe1wGP7fsk0kxkby03Nmeju0YcVERnDunBxe3VxDS2ePX58b4GB7N+v3HhxVEipIi9cppEp5kS8TQR6wz+12lXXM3XRgklXqeq2IfH2wC4nItSKyRkTW1NfX+yjc8XHtTTCWcYIn1+xj7Z6D/OycmaTG+66sxFAunp9PV6+Dlz854Pfnfmd7PQ4DSzzoFnIpSI3TwWKlvMiTEhMneXJssIcOcmzg4HMkcAzweeBzwC9EZPoRDzLmHmPMAmPMgoyM4FzLlpEYQ3pCzKinkDa0dfG7l7dx3NRULjkmMJU75k5OYVp6PE+vrfb7c79dXk9qfPSoBsgLU+Np6eylucP/LRilJiJPWgR/8/DYQFXAZLfb+cD+Qc55xRjTboxpAN4B5nhw7aBUljv6vQl+8+JWOrp7+c2FR/tkNzJPiAgXH5PPR7ub/Nrl0j9ttCSdCJvnr91VjnqP7lamlFcMmQhE5AQR+QGQISI3u/38Cojw4NqrgRIRmSoi0cClOGsWuXsOOEVEIkXEDhwHbB3TKwkCZTlJbK9rpbvX4dH5KysbWLa+mutOLfLLArLhXDAvDxF4Zn2V355zY3UzTe3dLJkxukHqT9cS6DiBUt4wXIsgGkjA2X2T6PbTgnN18bCMMb3At3HuZbAVeMoYs1lErhOR66xztgKvABuBj4B/GGM2jf3lBNbMnER6+gw76kfe+auzp4+fP7uJwjQ7Ny4p9kN0w8tLieOEaWk8s64aY/xTvmFFeR0isKhkdN19kydZawl05pBSXjHc9NG3gbdF5AHXSmIRsQEJ1taVIzLGvAS8NODYXQNu3wLcMtrAg9Est1ITrllEQ7lzxQ52NbTz0FULiY3ypIHlexfPz+cH//yY1bsPsnBqqs+fb3l5PfMmpzBplAPk8TGRpCfE6MwhpbzEkzGC/7U2r48HtgDlIvJDH8cVklw1/kcaJ9hZ38adK3Zw7pxcFk0PnsHvs47Kxh4dwTPrfN891NDWxcaqQ2Neu1CYZtcxAqW8xJNEUGa1AC7A+e2+ALjcp1GFqMgIGzOyE4edQmqM4efPbiImysYvvuDfNQMjiY+J5Kyjsnlx4wGv7sE8mHcq6jEGj8pKDKZQy1Er5TWeJIIoEYnCmQieM8b0MEwNonDn2ptgqH72ZzdUs2pHIz86awaZiYNvwBJIl8zPp7Wrl9e21Pr0eVaU15OeENPfnTZak1PtHGjppKvXtwlLqXDgSSK4G9gNxAPviEghzgFjNYiy3CQOdvQMWib5UEc3//PCVuZOTuGrCwsCEN3Ijp+WRm5yLE+v9V33UJ/D8HZFPYtLM7CNYtqou8I0O8ZA1cHDXo5OqfDjSRnqvxpj8owx5xinPcASP8QWklyDxIMtLPv9K9s4dLiH31549Jg/AH3NZhMunJ/Hu9vrqfVRzf8N+w7SfLiHxaNYTTyQawqpdg8pNX6erCzOEpF7ReRl63YZ8A2fRxaiZlhbTA4cMF6zu4nHP9rHVSdN6d/IJlhdND8fh4HnNvhmpfHybfVE2IRTiseeCCan6r4ESnmLJ11DD+BcC5Br3a4AvuergEJdYmwUhWn2zwwY9/Q5+H/LNpGbHMv3lh5RQSPoFGUkMK8ghafX+mZNwYqKOo4pmESyPWrM18hIiMEeHaFVSJXyguFWFrvWGKQbY54CHNC/UExH6IYxM/uzexP8491dlNe28qvzZvll5zFvuGh+PuW1rWz2wvab7upaOtlU3cLiETapH4mIUJBqZ58mAqXGbbgWwUfWf9tFJA1rppCIHA80+zqwUFaWm8Tuxnbau3rZ19TBX96s4IyyLM6clR3o0Dx27uwcoiNsPO3lNQUrKpzVYxdPH//eBwWpdi0zoZQXDJcIXKOZN+OsEVQkIiuBh4CbfB1YKJuZk4Qxzs1mfvn8Zmwi/Nd5swId1qik2KM5fWYmz2/YT0+fZ7WTPLGivI6spBhm5iSO+1oFqXb2NnXgcOhsZqXGY7hEkCEiNwOLgWXAH4CXgb8DS30fWuhyDQb/+Y3tvLWtjpvPmE5uSlyAoxq9i+fn09jezdvl3tkDoqfPwbvbG1hSmumVSquFaXa6eh3UtXZ5ITqlwtdwiSACZ9G5RJxrCCKtY3brmBpCbnIsSbGRvLu9gbKcJK44cUqgQxqTU0szSIuP9lr30Lo9B2nt7PXalpgFafGAFp9TaryGG7k8YIz5td8imUBEhLLcJMcI+SEAAB4NSURBVD7c1cRvLzqayAhfbgTnO1ERNs6bm8ujH+wd817MLq2dPfzulW3ERNo4qdg7ezIXuk0h9WaRvNe31FKWm0ReCLbilBoLT8YI1Bh857QSfn/xbOZO9nznrWB08fx8uvsc/Hvj2LexbO/q5cr7V7Oxqpm/XDqXxNixTxt1l5sSh0282yJoau/m2ofXcPk/PqT5sO6ApsLDcIngdL9FMQGdWJzOlxZMHvnEIDcrN4kZ2YljLjnR0d3LlQ+sZv2+Q/z10nmcdVSO12KLjrSRmxLn1UTwwc5GjIGdDe1894n19OlAtAoDQyYCY0yTPwNRwUlEuGh+Hhv2HfJowx13h7v7uPqBNazZ3cStX57L52d7Lwm4FKZ5dwrpysoGEmIi+fX5s1hRXs8fXt3mtWsrFaxCs/Na+dUFc/OwCaPap6Czp49vPrSGD3Y18qcvzeW8ObkjP2gMClLjvdoiWLWjkYVTU/n6CVP46nEF3P32Tp+V2lAqWGgiUCPKTIpl0fQMlq2r9mjOfmdPH9c+vJaVOxq45ZI5XDAvz2exFaTaaWrvprVz/P35+w8dZldDOycWOQezf3nuLBZOTeVH/9rIxqpD476+UsFKE4HyyEXz89nf3MkHOxuHPa+rt4/rH1nLOxX1/P6i2VxyTL5P4+qvQuqFVsGqHc7XdlJxOuAcg7jjq/NJT4jhWw+vpa7VN9VYlQo0TQTKI2eWZZEYG8m/huke6u51cOOj61heXs9vLzyaLx3r+8HyglTvlaNeVdlAWnw0pVmfLpNJT4jhnq8fw8GObq5/ZJ1uhKMmJE0EyiOxURF8YXYOr2yqob2r94j7e/oc3PjYOt7YWsd/X3AUXznOPxvvFFgtgvFWITXGsGpHI8cXpR2xV8Ss3GT++MU5rN1zkP98drNPKrIqFUiaCJTHLpqfT0d3H69sqvnM8Z4+B995fD2vb6nlV+eWcfnxhX6LKSk2ikn2qHHPHNrZ0E5NSycnFaUPev8XZufy7SXFPLlmHw9/sGdcz6VUsNFEoDy2oHAShWn2z5Sc6O1z8L0nN/Dyphp+/vmZXHHSVL/HVZAWP+5y1KsqGwCGXfV88xnTWTozk//69xZW7WgY1/MpFUw0ESiPiQgXzcvn/Z2NVB86TJ/DcPNTH/PixgP87JwZXHPKtIDEVZhqZ0/T+HYqW1nZSF5KXP+Yw2BsNuHWL89lano8Nz66TvdCUBOGJgI1KhfNz8MYeGZtFT/858c8//F+fnRWKdcuKgpYTAWpdvYf6hxzuWyHw/D+zkZOLEobsSpqYmwUf//6Avochm8+tGbQ8RKlQo0mAjUqk1PtLJyayp/f3M4z66v5jzOnc8Pi4oDGVJBmp89hqD54eEyP33KghebDPf3TRkcyNT2e274yn4raVv7jnx/rfggq5GkiUKN26bGT6XMYvre0hG+fVhLocPqrkI51LcFKa3zghCLPq6Iump7Bz86ZycubavjbW5Vjel6lgkVobKCrgsqF8/KYVzCJKWlD96f7U6G1L8FYp5Cu2tFIcWYCWUmxo3rc1SdPZcv+Fm59o4IZOYl8LoS2IlXKnbYI1KiJCFPT472yy5g3ZCbGEB1pY2/j6AeMu3sdfLSriZNG0RpwERF+e9HRzMlP5uYnN1Be0zrs+cYYWjt7qD50mC37W3h/RyOvbq7h9S21ujZBDcsYwy2vbmNTtW+2i9cWgQp5NpuMeSP7DfsOcbinjxOGWD8wktioCO6+fAHn3vYe1zy0mnOOzqHlcA8th3tpPtxDS2cPzYedPy2HexhqOOGurx3DWUdpi0IdyRjD/768jXve2YkgHJWX7PXn0ESgJoRCayP70Vq1owGbwAnTxr5rWnZyLHdffgxXPbCa+9/bTVJcFMlxkSTFRZEaH83U9HiSYqNIjnP+JMVFWv+NIik2ihseXcddb+/gc7OygqaVpYLHn16v4J53dnL58YX84MzpPnkOTQRqQihIs/P+zkaMMaP6MF1V2chReckk28e3a9r8gkms+/kZiDDqD/NvLprGL57dxIe7mjh+HAlJTTx/e3M7f3urkkuPncx/nTfLZ18UdIxATQgFqXY6uvtoaOv2+DEd3b2s33dwVLOFhmOzyZj+oX7xmHzSE6K56+0dXolDTQz3vLOD/3u9govm5fGbC48+ogaWN2kiUBPCp+WoPR8wXr37ID19Zsj6Qv4SGxXBFSdOYUV5PVsPtAQ0FhUcHli5i9++tI3Pz87hD5fMJsKHSQA0EagJoiDVOYV0NOMEqyobiI6wceyUVF+F5bHLj59CfHQEd2urIOw99uFefvXvLZxZlsWfvzyXyAjff0xrIlATQv6kOEQY1cyhlTsamFeQQlx0hA8j80yyPYrLFhbw740HqDqoNYzC1b/WVvGzZZ+wpDSDv31lHlF+SAKgiUBNELFREWQnxXq8Qc2hjm4272/hxAB3C7m7+pSp2AT+8e6uQIeiAuC5DdX86F8fc0pJOnd+7RhiIv33BcWniUBEzhKRchGpFJGfDHPesSLSJyKX+DIeNbEVpNo9Xl38wc5GjBm+7LS/5STHcf7cPJ5cvY+D7Z4PeqvQ9/InB7j5qY85dkoq91y+gNgo/7ZSfZYIRCQCuB04GygDLhORsiHO+z3wqq9iUeGhMM3ztQQrKxuxR0cwZ3KKj6ManW8tmsbhnj4efH93oENRfvLGllpuenw9cyencN8Vxwakq9KXLYKFQKUxZqcxpht4Ajh/kPNuAp4G6nwYiwoDhWnx1Ld20dE9cmnolTsaWDg11W99sJ4qyUpk6cxMHly126PXoULb2xX13PDoOmblJnH/lccSHxOYpV2+/FeQB+xzu11lHesnInnAhcBdw11IRK4VkTUisqa+vt7rgaqJYbKHVUhrmjvZWd8e8GmjQ7nu1CIOdvTw1Op9I5+sQtaqHQ1c+9AaijMTeOiq40iKHd+ixvHwZSIYbOLrwEorfwZ+bIzpG+5Cxph7jDELjDELMjIyvBagmlj6y1GPMGDs2mbyxCAaH3C3YEoqCwon8fd3d415sx0V3FbvbuLqB9ZQmGbnkWuOG/fK9vHyZSKoAia73c4H9g84ZwHwhIjsBi4B7hCRC3wYk5rAPl1UNnwiWFnZyCR7FDOzk/wR1phcd2oR1YcO8+LGA4EORXnZ+r0HufL+1eSkxPLoNceTGh8d6JB8mghWAyUiMlVEooFLgefdTzDGTDXGTDHGTAH+BdxgjHnWhzGpCSw5LorE2Mhh1xIYY3h/RwMnFKX5dMn+eJ02I5OSzATuenuHlqieQIwxfP/JDaTGR/PYNceTkRgT6JAAHyYCY0wv8G2cs4G2Ak8ZYzaLyHUicp2vnleFLxGhMG34KaS7GzvY39wZVOsHBmOzCd86tYhtNa2sqNBxsYli3d5D7G7s4Dunl5CdPLqNkHzJp1MmjDEvGWOmG2OKjDG/sY7dZYw5YnDYGHOFMeZfvoxHTXyFqfHsGyYRuLalPNFLheZ86bw5ueQkx3LXCi07MVEsW19FbJQt6PaeCK65c0qNU0GanaqDHfQNsQPM+zsayUmOZWp6vJ8jG73oSBtXnzyVD3c1sX7vwUCHo8apu9fBCxsPcGZZNgkBmiY6FE0EakIpSLXT02fYf+jwEfc5HIZVOxo4sSg9ZDaAuXRhAUmxkdz99s5Ah6LGaUV5HYc6erhwXt7IJ/uZJgI1oRQOs5Zga00LBzt6gqqsxEgSYiL5+glTeHVLDTvq2wIdTlDbvL+Z8297j+pBvgQEg2c3VJMWH80pJcE3PqWJQE0oBcNMIX1/RyNA0A8UD3TFSVOIjrDx93e0VTCcW1+v4OOqZu5YXhnoUI7QfLiHN7bWce6cXL+UlR6t4ItIqXHISY4jKkIGnUK6srKBaRnxQTVbwxPpCTF8cUE+z6yrpq6lM9DhBKVtNS28sbWO1Pho/rmmiprm4HqfXv7kAN29Di6aH3zdQqCJQE0wETYhf5L9iJ3KevocfLSrKWjLSozk2lOK6HU4uHellqgezJ0rdhAfHcFDVy3EYUzQbfu5bH010zLiOTovOdChDEoTgZpwClLtR7QINlYdor27LySmjQ6mIM3OOUfn8NgHe2np7Al0OEFlT2M7//54P189vpCj8pK5cF4ej3+0l7rW4GgVVB3s4MNdTVw4Ny9oJyloIlATTmGanb2NHZ9ZkbuyshERvLZRfSBcd2oRrV29PPrB3kCHElTufmcnkTYb15w8FYAblxTT0+cImg1+ntvgrKxzQRDOFnLRRKAmnIJUO61dvRzq+PSb88rKBmblJpFiD3xdl7E6Ki+ZU0rSuW/lLjp7hq3TGDZqWzr515oqLlmQT2aSc+xnSno858/N4+H399DY1hXQ+IwxLFtfzbFTJvVXxw1GmgjUhFNg/YNzlZo43N3H+r2HQm620GC+taiI+tYulq2vDnQoQeHe93bR63Bw3aKizxy/cUkxnb193PteYFsFm/e3UFnXxoXz8gMax0g0EagJpzDNuWp4T6NzwHjNnia6+xwhOz7g7qTiNI7KS+Ked3YOuXo6XBzq6OaRD/Zw3pzc/mnDLsWZCZxzdA4Pvb+HQx2B2/bzmXXVREfY+PzROQGLwROaCNSE42oRuGoOraxsJNImLJyaGsiwvEJEuO7UInY1tPP6lppAhxNQD6zaTUd3H9cvLh70/ptOK6atq5f7V+72b2CW3j4Hz3+8n9NmZAZ8v4GRaCJQE05cdASZiTH9M4dW7WhgXkEK9ujgqu8yVmcflUNhmp3bl++guzc8N65p7+rlgVW7WTozi9LsxEHPmZGdxJllWdy/chetAZhptXJHIw1tXUE9SOyiiUBNSAWpznLUzR09bKpunhDjAy4RNuH7S6fzSXUzV9z/Ec2Hw2866eMf7eVQRw83LCka9rybTiuhpbOXh97f46fIPrVsXRXJcVEsmRH8uypqIlATUoE1hfSDXY04DJxUPHESATinIt765Tms3t3ExXeuGrb09kTT1dvH39/dyQnT0phfMGnYc4/OT2ZJaQb/eHcn7V29forQ2WJ5dXMtn5+dQ0xkhN+ed6w0EagJqTA1npqWTpZvqyMuKoK5k1MCHZLXXTgvn4evPo761i4uvGNl2JSqfmZdNbUtXdy4ZPCxgYFuOr2Egx09PPqh/1oFr26u4XBPX1BWGh2MJgI1IRWkxQHw74/3c+zUVKIjJ+af+vHT0njmhhOxR0dy6T0f8Mqmib3HcW+fg7ve3sGc/GSPq8jOL5jEycXp3POO/9ZfLFtfTf6kOBYUDt9iCRYT81+HCnsFqc4ppO3dfZw0AaaNDqcoI4FlN5zIrNwkrn90HX9/Z6dX9zmuae7kty9t5bkNgV+78OInB9jT2MH1i4tHVa7hptOKaWjr4vGPfL8qu66lk5WVDVw4L3hLSgw0MaZRKDVAodu88ok2PjCYtIQYHvvm8fzgqY/5zUtb2d3Yzn+dN2tcJY/rWju5a8VOHvlwD929DhJiIllUksGk+MCszjbGcOeKHRRnJnBmWdaoHnvctDQWTk3l7rd38pXjCnzab//8x/txmOAuKTGQtgjUhJQWH018dATJcVHMzEkKdDh+ERsVwd8um8d1pxbx6Id7ueahNbSNYYC0sa2L/31pK4v+sJwH39/N+XNyue+KBbR393J3APdEeGtbHdtqWrlhcRE22+i/aX/39BJqWjr555oqH0T3qWXrq5mTn0xRRoJPn8ebtEWgJiQR4ej8ZPJS7ESM4UMjVNlswk/OnkFhmp2fP7uJS+5cxf1XHktOctyIjz3U0c3f393J/St309nTx/lz8/jO6SX9+zufNyeXB1ft5uqTp5KRGOPrl/IZxhhuX15JXkoc587JHdM1TixKY35BCneu2MGXj51MlA82iKmobWXz/hZ+eW6Z16/tS9oiUBPWg1ct5HcXHx3oMALisoUF3H/FsVQdPMwFt69kU3XzkOc2H+7h1tcrOOX3y7ljxQ5Om5HJa99fxK1fntufBMD5jbq7z8GdK/xf6//DXU2s23uI606dNuYPcBHhptNLqD50mGXrfDPesWx9NRE2GXOyChRNBGrCiomM8Mm3vlCxaHoG/7r+BCJE+NLd77N8W91n7m/r6uW2t7Zzyu/f4i9vbuek4nRe/u4p3PaV+RRnHrlad1pGAhfPz+ORD/dwoNm/+wLfvrzS2qlt8rius3h6BrPzk7lteSW9fd5dle1wGJ5bX82iknTSE/zbYhqv8P1XolQYmJGdxLIbT2JaRjxXP7iah9/fTUd3L3e9vYNTfv8Wf3ytgoVT03jhppO56/JjmJE9/HjKTaeVYIzhtrf8ty/wJ1XNvLu9gatPnkps1PgGeUWEby8pZm9TB89/vN9LETp9uKuJ/c2dXDg/uCuNDkYTgVITXFZSLE9eewKnzcjkF89t5rjfvMnvXt7GnMkpPHfjSfzjGws4ysMtFCen2rn02AKeXL3Pb6uZ71hRSWJsJF87vsAr1zujLIsZ2YnctrzSqxVcn11fTUJMJGfMHN2MpmCgiUCpMBAfE8ndly/gW6dO49ipqTx9/Qk8cOVC5oxhxfWNS4qx2YS/vLndB5F+VmVdK69sruEbJ0whMdY7FTxFhJtOK2FnfTsvfeKdBXidPX289MkBzjoqm7jo4C8pMZAmAqXCRIRN+OnZM7nvimM5pnDsJbmzk2O5/PhCnllXxc76Ni9GeKQ7V+wkJtLGlSdN8ep1zz4qm+LMBG57qxKHF1oFb26to7WrN2RKSgykiUApNWrXLy4iJjKCP7/hu1ZB1cEOnttQzWULC0jz8uCrzeYcKyivbeW1LbXjvt6y9VVkJcVw/LTQXMWuiUApNWrpCTFcedIU/r1xP9tqWnzyHH9/Zyci8M1Tpvnk+l+YncOUNDt/e2v7uEpyNLV3s6K8ngvm5oXsmhVNBEqpMbl20TQSoiO59fUKr1+7vrWLJ1bv48J5eeSmjLwYbiwiI2zcsKSYzftbWF5eN/IDhvDixv30OkxIlZQYSBOBUmpMUuzRXH3KVF7dXMsnVUMvWBuL+1fuorvPwXWnDr/xzHhdOC+P/Elx3PJqBRv2HRpTy+CZ9dXMyE4M6VImmgiUUmN21clTSbFH8afXy712zX1NHTz8/h7OOSqHaT6u1xMVYeOnZ8+ksq6VC25fycm/X85vX9rqcVLY3dDO+r2HQnaQ2EVrDSmlxiwpNopvLSri969sY+2epnHNRgLYXtvK1+79EJtN+P4ZJV6Kcnifn53DycXpvLalhpc+OcD9K3dxzzs7yZ8UxzlH5/D5o3OYnZ88aEnpZeurEYHz5oZWSYmBxJt1y/1hwYIFZs2aNYEOQyll6ejuZdEfljM9K5HHvnn8mK+zseoQ37jvI6IibDx89XFDbkrva80dPf1J4d3tDfQ6zKBJwRjD4j+uIH9SHI9eM/bX7S8istYYs2Cw+7RFoJQaF3t0JDcsLubXL2xh1Y4GTiwa/f4P7+9o5JsPrWFSfBSPXH0chWnxIz/IR5LtUXxxwWS+uGByf1J48ZMD3Pfepy2Fzx+dw7SMePY0dvBtD7fMDGbaIlBKjVtnTx+Lb3F+O/7ndSeMameuN7bUcsNj6yhMtfPw1ceRnRzrw0jH7lBHN69tqeWlTw7wntVSiI2ysfr/LfXaqmdfCliLQETOAv4CRAD/MMb8bsD9XwV+bN1sA643xnzsy5iUUt4XGxXBt08r5ufPbuLtinoWl2Z69LjnNlRz81MfMys3iQevXBiw3c88kWKP5ksLJvOlBZM51NHN61tqSY6LCokkMBKfzRoSkQjgduBsoAy4TEQG7tawCzjVGDMb+G/gHl/Fo5TyrS8tmEz+pDj+77UKj2bcPPzBHr735AaOnTKJR685LqiTwEAp9mi+uGAyZ87KDnQoXuHL6aMLgUpjzE5jTDfwBHC++wnGmFXGmIPWzQ+A0KvfqpQCIDrSxndPL+GT6uZhyza4dhv7xbObOH1GJg9cuXBCfKsOZb5MBHnAPrfbVdaxoVwNvDzYHSJyrYisEZE19fX1XgxRKeVNF87LY1p6PLe+XjFoMTdjDL97ZRu3vFrOBXNzufNrx4x7jwE1fr5MBIONFg3aXhSRJTgTwY8Hu98Yc48xZoExZkFGRoYXQ1RKeVNkhI3vLi1hW00rLw4o8dznMPxs2Sbufnsnlx9fyJ++NDesd5ALJr78v1AFuO8rlw8csSWQiMwG/gGcb4xp9GE8Sik/OHd2LqVZidz6RkX/dpDdvQ6++8R6Hv9oLzcuKeLX58/CFqIF2iYiXyaC1UCJiEwVkWjgUuB59xNEpAB4BrjcGOP9ylVKKb9zrgqezs76dp7dsJ/D3X1c+/AaXth4gJ+ePYMffm7GqKaXKt/z2fRRY0yviHwbeBXn9NH7jDGbReQ66/67gP8E0oA7rD+M3qHmuSqlQsfnZmVxVF4Sf3mzgqdW72P1nib+96KjuWyhd7abVN6lC8qUUj6xvLyOK+9fTaRNuPXLczl3TmjX4wl1WmJCKeV3i6dn8MPPlTJ3cgonFY++7ITyH00ESimfEBFunAB1eMKBzt1SSqkwp4lAKaXCnCYCpZQKc5oIlFIqzGkiUEqpMKeJQCmlwpwmAqWUCnOaCJRSKsyFXIkJEakH9ozx4elAgxfDmSj0fTmSvidH0vfkSKH0nhQaYwat4x9yiWA8RGSNFrU7kr4vR9L35Ej6nhxporwn2jWklFJhThOBUkqFuXBLBPcEOoAgpe/LkfQ9OZK+J0eaEO9JWI0RKKWUOlK4tQiUUkoNoIlAKaXCXNgkAhE5S0TKRaRSRH4S6HiCgYjsFpFPRGSDiITt/p8icp+I1InIJrdjqSLyuohst/47KZAx+tsQ78mvRKTa+nvZICLnBDJGfxKRySKyXES2ishmEfmudXxC/J2ERSIQkQjgduBsoAy4TETKAhtV0FhijJk7EeZCj8MDwFkDjv0EeNMYUwK8ad0OJw9w5HsCcKv19zLXGPOSn2MKpF7gB8aYmcDxwI3WZ8iE+DsJi0QALAQqjTE7jTHdwBPA+QGOSQUJY8w7QNOAw+cDD1q/Pwhc4NegAmyI9yRsGWMOGGPWWb+3AluBPCbI30m4JII8YJ/b7SrrWLgzwGsislZErg10MEEmyxhzAJwfAkBmgOMJFt8WkY1W11FIdoOMl4hMAeYBHzJB/k7CJRHIIMd03iycZIyZj7PL7EYRWRTogFRQuxMoAuYCB4D/C2w4/iciCcDTwPeMMS2BjsdbwiURVAGT3W7nA/sDFEvQMMbst/5bByzD2YWmnGpFJAfA+m9dgOMJOGNMrTGmzxjjAP5OmP29iEgUziTwqDHmGevwhPg7CZdEsBooEZGpIhINXAo8H+CYAkpE4kUk0fU7cCawafhHhZXngW9Yv38DeC6AsQQF1wee5ULC6O9FRAS4F9hqjPmT210T4u8kbFYWW1Pd/gxEAPcZY34T4JACSkSm4WwFAEQCj4XreyIijwOLcZYUrgV+CTwLPAUUAHuBLxpjwmbwdIj3ZDHObiED7Aa+5eofn+hE5GTgXeATwGEd/hnOcYKQ/zsJm0SglFJqcOHSNaSUUmoImgiUUirMaSJQSqkwp4lAKaXCnCYCpZQKc5oIlPIxEVksIi8EOg6lhqKJQCmlwpwmAqUsIvI1EfnIqrV/t4hEiEibiPyfiKwTkTdFJMM6d66IfGAVYFvmKsAmIsUi8oaIfGw9psi6fIKI/EtEtonIo9ZKVUTkdyKyxbrOHwP00lWY00SgFCAiM4Ev4yzENxfoA74KxAPrrOJ8b+NcYQvwEPBjY8xsnKtNXccfBW43xswBTsRZnA2c1Sq/h3M/jGnASSKSirNUwyzrOv/j21ep1OA0ESjldDpwDLBaRDZYt6fhLCfwpHXOI8DJIpIMpBhj3raOPwgssmo35RljlgEYYzqNMR3WOR8ZY6qsgm0bgClAC9AJ/ENELgJc5yrlV5oIlHIS4EG33bdKjTG/GuS84WqyDFbu3KXL7fc+INIY04uzgufTODc0eWWUMSvlFZoIlHJ6E7hERDKhfy/aQpz/Ri6xzvkK8J4xphk4KCKnWMcvB9626tNXicgF1jViRMQ+1BNate2TrS0fv4ezoJtSfhcZ6ACUCgbGmC0i8nOcO7bZgB7gRqAdmCUia4FmnOMI4Cw5fJf1Qb8TuNI6fjlwt4j82rrGF4d52kTgORGJxdma+L6XX5ZSHtHqo0oNQ0TajDEJgY5DKV/SriGllApz2iJQSqkwpy0CpZQKc5oIlFIqzGkiUEqpMKeJQCmlwpwmAqWUCnP/H2lbzgBcLYcgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.ylabel(\"Test loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ticklabel_format(axis = \"y\", style = \"sci\", scilimits=(0,0))\n",
    "plt.title(\"Training loss over epochs\")\n",
    "plt.savefig(\"loss.png\", dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trial5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"trial2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_array = np.empty([3200, 60], dtype = \"float\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j in range(len(val_dataset)):\n",
    "        current_scene = val_dataset[j]\n",
    "        current_scene_id = int(current_scene[\"scene_idx\"])\n",
    "        target_veh = np.where(current_scene[\"track_id\"][:, 0 , 0] == current_scene[\"agent_id\"])[0]\n",
    "        input_seq = torch.FloatTensor(numpy.dstack([current_scene['p_in'], current_scene['v_in']])).cuda()\n",
    "        y_pred_output = model.forward_test(input_seq.transpose(1,2).reshape(-1, 19, 120))\n",
    "        y_pred_output = y_pred_output.reshape((-1, 30, 60, 2)).transpose(1, 2)\n",
    "        #print(target_veh)\n",
    "        #print(y_pred_output[0, 3, :, :].cpu().detach().numpy().flatten())\n",
    "        output_veh = y_pred_output[0, target_veh, :, :].cpu().detach().numpy().flatten()\n",
    "        #print(output_veh)\n",
    "        output_array[j] = output_veh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 30, 4])\n",
      "tensor([[[ 1.8913e+03,  1.2827e+03,  4.3181e-01,  8.2107e-03],\n",
      "         [ 2.0447e+03,  1.2594e+03,  4.9185e-01,  2.1091e-01],\n",
      "         [ 2.1346e+03,  1.2548e+03,  5.2664e-01,  2.8830e-01],\n",
      "         [ 2.1786e+03,  1.2564e+03,  5.4430e-01,  3.1080e-01],\n",
      "         [ 2.2039e+03,  1.2589e+03,  5.5566e-01,  3.2205e-01],\n",
      "         [ 2.2230e+03,  1.2615e+03,  5.6504e-01,  3.3027e-01],\n",
      "         [ 2.2392e+03,  1.2639e+03,  5.7308e-01,  3.3696e-01],\n",
      "         [ 2.2536e+03,  1.2659e+03,  5.8020e-01,  3.4220e-01],\n",
      "         [ 2.2701e+03,  1.2678e+03,  5.8842e-01,  3.4595e-01],\n",
      "         [ 2.2989e+03,  1.2700e+03,  6.0445e-01,  3.4879e-01],\n",
      "         [ 2.3356e+03,  1.2733e+03,  6.2419e-01,  3.5299e-01],\n",
      "         [ 2.3658e+03,  1.2772e+03,  6.3869e-01,  3.5769e-01],\n",
      "         [ 2.4001e+03,  1.2788e+03,  6.6241e-01,  3.7296e-01],\n",
      "         [ 2.4486e+03,  1.2797e+03,  6.9452e-01,  3.9812e-01],\n",
      "         [ 2.4773e+03,  1.2791e+03,  7.0355e-01,  4.1517e-01],\n",
      "         [ 2.4814e+03,  1.2765e+03,  6.9693e-01,  4.2177e-01],\n",
      "         [ 2.4816e+03,  1.2744e+03,  6.9258e-01,  4.2280e-01],\n",
      "         [ 2.4930e+03,  1.2754e+03,  6.9808e-01,  4.2059e-01],\n",
      "         [ 2.5054e+03,  1.2766e+03,  7.0121e-01,  4.2092e-01],\n",
      "         [ 2.5100e+03,  1.2763e+03,  6.9691e-01,  4.2345e-01],\n",
      "         [ 2.5113e+03,  1.2759e+03,  6.9238e-01,  4.2489e-01],\n",
      "         [ 2.5120e+03,  1.2757e+03,  6.8932e-01,  4.2550e-01],\n",
      "         [ 2.5125e+03,  1.2757e+03,  6.8710e-01,  4.2568e-01],\n",
      "         [ 2.5130e+03,  1.2756e+03,  6.8507e-01,  4.2561e-01],\n",
      "         [ 2.5135e+03,  1.2755e+03,  6.8285e-01,  4.2532e-01],\n",
      "         [ 2.5139e+03,  1.2755e+03,  6.8029e-01,  4.2493e-01],\n",
      "         [ 2.5143e+03,  1.2753e+03,  6.7729e-01,  4.2445e-01],\n",
      "         [ 2.5147e+03,  1.2752e+03,  6.7384e-01,  4.2388e-01],\n",
      "         [ 2.5150e+03,  1.2750e+03,  6.6999e-01,  4.2329e-01],\n",
      "         [ 2.5154e+03,  1.2748e+03,  6.6580e-01,  4.2271e-01],\n",
      "         [ 1.8924e+03,  1.2838e+03,  5.2586e-01, -3.4797e-01],\n",
      "         [ 2.0459e+03,  1.2606e+03,  6.1247e-01, -1.0297e-01],\n",
      "         [ 2.1358e+03,  1.2561e+03,  6.6548e-01,  4.0171e-03],\n",
      "         [ 2.1798e+03,  1.2577e+03,  6.9024e-01,  4.0897e-02],\n",
      "         [ 2.2051e+03,  1.2603e+03,  7.0452e-01,  6.0179e-02],\n",
      "         [ 2.2243e+03,  1.2629e+03,  7.1574e-01,  7.4270e-02],\n",
      "         [ 2.2404e+03,  1.2653e+03,  7.2504e-01,  8.5920e-02],\n",
      "         [ 2.2548e+03,  1.2673e+03,  7.3297e-01,  9.5839e-02],\n",
      "         [ 2.2714e+03,  1.2692e+03,  7.4186e-01,  1.0536e-01],\n",
      "         [ 2.3002e+03,  1.2714e+03,  7.5930e-01,  1.1859e-01],\n",
      "         [ 2.3369e+03,  1.2748e+03,  7.8059e-01,  1.3504e-01],\n",
      "         [ 2.3671e+03,  1.2787e+03,  7.9576e-01,  1.4811e-01],\n",
      "         [ 2.4014e+03,  1.2804e+03,  8.2778e-01,  1.7259e-01],\n",
      "         [ 2.4500e+03,  1.2813e+03,  8.7915e-01,  2.1017e-01],\n",
      "         [ 2.4787e+03,  1.2808e+03,  9.0133e-01,  2.3280e-01],\n",
      "         [ 2.4828e+03,  1.2782e+03,  9.0022e-01,  2.3807e-01],\n",
      "         [ 2.4830e+03,  1.2761e+03,  8.9836e-01,  2.3835e-01],\n",
      "         [ 2.4943e+03,  1.2772e+03,  9.0640e-01,  2.4038e-01],\n",
      "         [ 2.5068e+03,  1.2784e+03,  9.1317e-01,  2.4440e-01],\n",
      "         [ 2.5114e+03,  1.2780e+03,  9.1187e-01,  2.4648e-01],\n",
      "         [ 2.5127e+03,  1.2777e+03,  9.0910e-01,  2.4655e-01],\n",
      "         [ 2.5134e+03,  1.2775e+03,  9.0710e-01,  2.4628e-01],\n",
      "         [ 2.5139e+03,  1.2775e+03,  9.0567e-01,  2.4602e-01],\n",
      "         [ 2.5144e+03,  1.2774e+03,  9.0437e-01,  2.4573e-01],\n",
      "         [ 2.5149e+03,  1.2773e+03,  9.0296e-01,  2.4532e-01],\n",
      "         [ 2.5153e+03,  1.2773e+03,  9.0131e-01,  2.4480e-01],\n",
      "         [ 2.5157e+03,  1.2771e+03,  8.9936e-01,  2.4412e-01],\n",
      "         [ 2.5161e+03,  1.2770e+03,  8.9714e-01,  2.4323e-01],\n",
      "         [ 2.5165e+03,  1.2768e+03,  8.9464e-01,  2.4220e-01],\n",
      "         [ 2.5169e+03,  1.2766e+03,  8.9187e-01,  2.4109e-01]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f22a8fba5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    }
   ],
   "source": [
    "output_array = np.empty([3200, 60], dtype = \"float\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i_batch, sample_batch in enumerate(val_loader):\n",
    "        sample_batch = sample_batch.cuda()\n",
    "        output = model.forward_test(sample_batch.transpose(1,2).reshape(-1, 19, 240))\n",
    "        y_pred_output = y_pred_output.reshape((-1, 30, 60, 4)).transpose(1, 2)\n",
    "        print(y_pred_output.shape)\n",
    "        print(y_pred_output.cpu().detach()[:, :, 0, :])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_array = []\n",
    "for j in range(len(val_dataset)):\n",
    "    current_scene = val_dataset[j]\n",
    "    current_scene_id = int(current_scene[\"scene_idx\"])  \n",
    "    ID_array.append(current_scene_id)\n",
    "    \n",
    "first_row = numpy.array([\"ID\"])\n",
    "for i in range(1, 61):\n",
    "    first_row = np.append(first_row, \"v\" + str(i))\n",
    "output_pd = pd.DataFrame(data = output_array, index = ID_array)\n",
    "output_pd = output_pd.reset_index()\n",
    "output_pd.columns = first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>2035.125854</td>\n",
       "      <td>2712.453613</td>\n",
       "      <td>1711.908691</td>\n",
       "      <td>2476.324219</td>\n",
       "      <td>1346.075684</td>\n",
       "      <td>2434.213379</td>\n",
       "      <td>1184.438354</td>\n",
       "      <td>2302.698242</td>\n",
       "      <td>1018.390076</td>\n",
       "      <td>...</td>\n",
       "      <td>821.567810</td>\n",
       "      <td>2220.396240</td>\n",
       "      <td>822.353821</td>\n",
       "      <td>2220.786377</td>\n",
       "      <td>822.761292</td>\n",
       "      <td>2220.801758</td>\n",
       "      <td>822.586731</td>\n",
       "      <td>2220.362793</td>\n",
       "      <td>822.430115</td>\n",
       "      <td>2219.978271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10015</td>\n",
       "      <td>1977.782715</td>\n",
       "      <td>2776.608154</td>\n",
       "      <td>1679.162354</td>\n",
       "      <td>2551.137207</td>\n",
       "      <td>1343.995972</td>\n",
       "      <td>2500.806885</td>\n",
       "      <td>1182.248291</td>\n",
       "      <td>2350.958008</td>\n",
       "      <td>1029.140503</td>\n",
       "      <td>...</td>\n",
       "      <td>737.996521</td>\n",
       "      <td>2330.256592</td>\n",
       "      <td>738.211243</td>\n",
       "      <td>2330.746338</td>\n",
       "      <td>738.425476</td>\n",
       "      <td>2331.235352</td>\n",
       "      <td>738.646729</td>\n",
       "      <td>2331.718262</td>\n",
       "      <td>738.911133</td>\n",
       "      <td>2332.182129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10019</td>\n",
       "      <td>2161.729736</td>\n",
       "      <td>2870.184082</td>\n",
       "      <td>1847.937012</td>\n",
       "      <td>2627.224854</td>\n",
       "      <td>1451.998901</td>\n",
       "      <td>2575.147705</td>\n",
       "      <td>1292.328857</td>\n",
       "      <td>2434.724121</td>\n",
       "      <td>1143.206299</td>\n",
       "      <td>...</td>\n",
       "      <td>859.488770</td>\n",
       "      <td>2399.432129</td>\n",
       "      <td>859.717590</td>\n",
       "      <td>2399.860107</td>\n",
       "      <td>859.943359</td>\n",
       "      <td>2400.284912</td>\n",
       "      <td>860.164246</td>\n",
       "      <td>2400.711914</td>\n",
       "      <td>860.380859</td>\n",
       "      <td>2401.126221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10028</td>\n",
       "      <td>2210.946289</td>\n",
       "      <td>2628.141602</td>\n",
       "      <td>2042.605713</td>\n",
       "      <td>2461.962891</td>\n",
       "      <td>1681.505371</td>\n",
       "      <td>2291.802002</td>\n",
       "      <td>1436.556274</td>\n",
       "      <td>2196.534424</td>\n",
       "      <td>1253.301514</td>\n",
       "      <td>...</td>\n",
       "      <td>1045.287842</td>\n",
       "      <td>2315.906738</td>\n",
       "      <td>1045.234009</td>\n",
       "      <td>2316.316895</td>\n",
       "      <td>1044.515381</td>\n",
       "      <td>2316.976074</td>\n",
       "      <td>1043.471313</td>\n",
       "      <td>2317.687012</td>\n",
       "      <td>1042.884766</td>\n",
       "      <td>2318.061035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>1823.865845</td>\n",
       "      <td>2554.524414</td>\n",
       "      <td>1542.870850</td>\n",
       "      <td>2350.400879</td>\n",
       "      <td>1227.935547</td>\n",
       "      <td>2269.351318</td>\n",
       "      <td>1070.217529</td>\n",
       "      <td>2091.079834</td>\n",
       "      <td>946.335083</td>\n",
       "      <td>...</td>\n",
       "      <td>696.935181</td>\n",
       "      <td>2087.850342</td>\n",
       "      <td>697.296814</td>\n",
       "      <td>2088.475830</td>\n",
       "      <td>697.648132</td>\n",
       "      <td>2089.078125</td>\n",
       "      <td>697.988403</td>\n",
       "      <td>2089.661621</td>\n",
       "      <td>698.319092</td>\n",
       "      <td>2090.225586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID           v1           v2           v3           v4           v5  \\\n",
       "0  10002  2035.125854  2712.453613  1711.908691  2476.324219  1346.075684   \n",
       "1  10015  1977.782715  2776.608154  1679.162354  2551.137207  1343.995972   \n",
       "2  10019  2161.729736  2870.184082  1847.937012  2627.224854  1451.998901   \n",
       "3  10028  2210.946289  2628.141602  2042.605713  2461.962891  1681.505371   \n",
       "4   1003  1823.865845  2554.524414  1542.870850  2350.400879  1227.935547   \n",
       "\n",
       "            v6           v7           v8           v9  ...          v51  \\\n",
       "0  2434.213379  1184.438354  2302.698242  1018.390076  ...   821.567810   \n",
       "1  2500.806885  1182.248291  2350.958008  1029.140503  ...   737.996521   \n",
       "2  2575.147705  1292.328857  2434.724121  1143.206299  ...   859.488770   \n",
       "3  2291.802002  1436.556274  2196.534424  1253.301514  ...  1045.287842   \n",
       "4  2269.351318  1070.217529  2091.079834   946.335083  ...   696.935181   \n",
       "\n",
       "           v52          v53          v54          v55          v56  \\\n",
       "0  2220.396240   822.353821  2220.786377   822.761292  2220.801758   \n",
       "1  2330.256592   738.211243  2330.746338   738.425476  2331.235352   \n",
       "2  2399.432129   859.717590  2399.860107   859.943359  2400.284912   \n",
       "3  2315.906738  1045.234009  2316.316895  1044.515381  2316.976074   \n",
       "4  2087.850342   697.296814  2088.475830   697.648132  2089.078125   \n",
       "\n",
       "           v57          v58          v59          v60  \n",
       "0   822.586731  2220.362793   822.430115  2219.978271  \n",
       "1   738.646729  2331.718262   738.911133  2332.182129  \n",
       "2   860.164246  2400.711914   860.380859  2401.126221  \n",
       "3  1043.471313  2317.687012  1042.884766  2318.061035  \n",
       "4   697.988403  2089.661621   698.319092  2090.225586  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1806.1553078021109"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(output_pd[\"v1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pd.to_csv(\"submission_523.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
